{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 276 Programming Assignment 2: Spelling Corrector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Overview\n",
    "\n",
    "In this assignment, we will build a probabilistic spelling corrector to automatically correct errors in queries. More formally, given a (possibly corrupt) raw query $R$, our goal is to find the intended query $Q$ which maximizes the probability $P(Q\\mid R)$. That is, we want to guess the query which the user probably meant to submit. By Bayes' Theorem we have\n",
    "$$\n",
    "    P(Q\\mid R) = \\frac{P(R\\mid Q)P(Q)}{P(R)}\\propto P(R\\mid Q)P(Q).\n",
    "$$\n",
    "Since our goal is to find the value of $Q$ which maximizes $P(Q\\mid R)$, this shows it is sufficient to maximize $P(R\\mid Q)P(Q)$. With the above formulation in mind, we will build a probabilistic spelling corrector consisting of 4 parts:\n",
    "  1. **Language Model.**\n",
    "      Estimates the prior distribution of unigrams and bigrams, allowing us to estimate $P(Q)$. We will use maximum-likelihood estimation, which counts the occurrences of token unigrams and bigrams in the training corpus in order to determine their prior probabilities.\n",
    "  2. **Edit Probability Model.**\n",
    "      Estimates the likelihood of errors that may occur in a query, which allows us to estimate $P(R\\mid Q)$. In particular, this component estimates the probability of characters being mistakenly deleted, inserted, substituted, or transposed in a query term.\n",
    "  3. **Candidate Generator.**\n",
    "      Takes a raw query $R$ submitted by the user, and generates candidates for $Q$.\n",
    "  4. **Candidate Scorer.**\n",
    "      Combines (1), (2), and (3) to compute $Q^{*} = \\arg\\max_{Q}P(Q\\mid R)$. That is, for each $Q$ generated by the candidate generator, the scorer uses the language model to estimate $P(Q)$ and uses the edit probability model to estimate $P(R\\mid Q)$, and finally chooses $Q$ which maximizes $P(Q)P(R\\mid Q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Assignment Details\n",
    "\n",
    "The assignment is due at **4:00 PM PST on Tuesday, May 7th, 2019**. We have split the assignment up into the following parts:\n",
    "  1. [Task 1: Spelling Correction with Uniform Edit Costs](#uniform): **55%** of your total grade for this assignment depends on a correctly implemented solution for task 1. Your solution will be evaluated on a hidden test set, and full credit will be given to models that are within 1% of the staff implementation's test-set accuracy or higher. We do not publish the test set queries or our accuracy on the test set. However, as a guideline for performance, the staff implementation with uniform edit probability model gets **82.42% on the dev set.** We will give partial credit on a non-linear scale (which disproportionately favors models that are closer to our threshold for full credit, as an encouragement to squeeze out more performance improvements).\n",
    "  2. [Task 2: Spelling Correction with Empirical Edit Costs](#empirical): **25%** of your total grade is based on your implementation of task 2. Full credit will be granted for accuracy levels within 1% of the staff implementation's test-set accuracy or higher. Again, we do not publish our test set accuracy, but the staff implementation with empirical edit probability model gets **87.91% on the dev set.** As with Task 1, we will give partial for lower accuracy levels, we will give partial credit on a non-linear scale, with credit accruing more rapidly as your solution gets closer to the target.\n",
    "  3. [Written Report](#written): **20%** of your grade is based on the 1-2 page report that you will submit through Gradescope. See [Section VI](#written) for instructions and grading breakdown.\n",
    "  4. [Extra Credit (Optional)](#extra): **Up to 10%** extra credit will be awarded for implementing extensions, with an explanation in the report. It is not necessary for the extensions to radically improve accuracy to get credit. As described in [Section VII](#extra), you can also get a small amount of extra credit if your system is a top performer in terms of accuracy or running time.\n",
    "\n",
    "The submission procedure is the same as in PA1, but we repeat the instructions here for your reference:\n",
    "  - This assignment should be done in teams of two or individually. Assignments are graded the same for one and two person teams.\n",
    "  - The notebook will automatically generate Python files in `submission` folder. To submit your assignment, **upload the Python files to the PA2-code assignment on Gradescope.** Note that you need to upload all the individual files in the `submission` folder without zipping it.\n",
    "  - While solving the assignment, do **NOT** change class and method names, otherwise the autograder tests will fail.\n",
    "  - You'll also have to **upload a PDF version of the notebook (which would be primarily used to grade your report section of the notebook) to PA2-PDF assignment on Gradescope.** Note that directly converting the PDF truncates code cells. To get a usable PDF version, first click on `File > Print Preview`, which will open in a new tab, then print to PDF using your browser's print functionality.\n",
    "  - After uploading the PDF make sure you tag all the relevant pages to each question. We reserve the right to penalize for mistagged submissions.\n",
    "  - If you are solving the assignment in a team of two, add the other student as a group member after submitting the assignment. Do **NOT** submit the same assignment twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Note on Numerical Stability\n",
    "\n",
    "Many of the probabilities we will encounter in this assignment are very small. When we multiply many small numbers together, there is a risk of [underﬂow](https://en.wikipedia.org/wiki/Arithmetic_underflow). Therefore, it is common practice to perform this type of probability calculation in log space. Recall that:\n",
    "  1. The log function is monotonically increasing, therefore $\\arg\\max p = \\arg\\max\\log p$.\n",
    "  2. We have $\\log(pq) = \\log p + \\log q$, and by extension $\\log\\left(\\prod_{i} p_i\\right) = \\sum_{i}\\log p_i$.\n",
    "\n",
    "As a result, if we want to maximize $P(\\textbf{x}) = P(x_1)P(x_2)\\cdots P(x_n)$, we can equivalently maximize $\\log P(\\textbf{x}) = \\log P(x_1) + \\log P(x_2) + \\cdots + \\log P(x_n)$. **For numerical stability, we recommend that you use this log-space formulation throughout the assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "## III. Dataset\n",
    "\n",
    "The dataset you will be working with for this assignment is available as a zip file at [this link](http://web.stanford.edu/class/cs276/pa/pa2-data.zip). The unzipped data directory will contain the following subdirectories:\n",
    "  - **Language Modeling Morpus (`pa2-data/corpus/`).** 99,904 documents crawled from the stanford.edu domain. The corpus is organized in a block structure found at `pa2-data/corpus/`, where you'll find 10 files. Each line in a file represents the text of a single document. You will use the tokens in these documents to build a language model.\n",
    "  - **Query Training Set (`pa2-data/training_set/`).** 819,722 pairs of misspelled queries and their corresponding corrected versions, with each pair separated by an edit distance of at most one. The two queries are tab-separated in the file `pa2-data/training_set/edit1s.txt`. You will use this data to build a probability model for the \"noisy channel\" of spelling errors.\n",
    "  - **Query Dev Set (`pa2-data/dev_set`).** 455 pairs of misspelled and corrected queries, which you will use to measure the performance of your model.  There are three files in `pa2-data/dev_set/`: the (possibly) misspelled queries are in `queries.txt`, corrected versions are in `gold.txt`, and Google's suggested spelling corrections are in `google.txt`.\n",
    "  \n",
    "Run the following code blocks to import packages, download, and unzip the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autograding_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/imports.py\n",
    "\n",
    "# Import modules\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "data_dir = 'pa2-data'\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/{}.zip'.format(data_dir)\n",
    "urllib.request.urlretrieve(data_url, '{}.zip'.format(data_dir))\n",
    "\n",
    "# Unzip dataset\n",
    "with zipfile.ZipFile('{}.zip'.format(data_dir), 'r') as zip_fh:\n",
    "    zip_fh.extractall()\n",
    "print('Data downloaded and unzipped to {}...\\n'.format(data_dir))\n",
    "\n",
    "# Print the directory structure\n",
    "print('Directory Structure:')\n",
    "print(data_dir + os.path.sep)\n",
    "for sub_dir in os.listdir(data_dir):\n",
    "    if not sub_dir.startswith('.'):\n",
    "        print('  - ' + sub_dir + os.path.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='uniform'></a>\n",
    "## IV. Task 1: Spelling Correction with Uniform Edit Costs (55%)\n",
    "\n",
    "### IV.1. Language Model\n",
    "\n",
    "We will now build a language model to estimate $P(Q)$ from the training corpus. We will treat $Q$ as a sequence of terms $(w_1, \\ldots, w_n)$ whose probability is computed as\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = P(w_1)P(w_2\\mid w_1)\\cdots P(w_n\\mid w_{n-1}),\n",
    "$$\n",
    "where $P(w_1)$ is the unigram probability of term $w_1$, and $P(w_{i}\\mid w_{i-1})$ is the bigram probability of $(w_{i-1}, w_i)$ for $i \\in \\{2, \\ldots, n\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.1.1. Calculating Unigram and Bigram Probabilities\n",
    "\n",
    "Our language model will use the maximum likelihood estimates (MLE) for both probabilities, which turn out to be their observed frequencies:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P_{\\text{MLE}}(w_i) & = \\frac{\\texttt{count}(w_i)}{T},\n",
    "    &\n",
    "    P_{\\text{MLE}}(w_i\\mid w_{i-1}) & = \\frac{\\texttt{count}((w_{i}, w_{i-1}))}{\\texttt{count}(w_{i-1})},\n",
    "\\end{align*}\n",
    "$$\n",
    "where $T$ is the total number of tokens in our corpus, and where $\\texttt{count}$ simply counts occurrences of unigrams or bigrams in the corpus. In summary, computing unigram probabilities $P(w_i)$ and bigram probabilities $P(w_{i}\\mid w_{i-1})$ is a simple matter of counting the unigrams and bigrams that appear throughout the corpus.\n",
    "\n",
    "Fill out the following code block to count the unigrams and bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/language_model_part1.py\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"Models prior probability of unigrams and bigrams.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus_dir='pa2-data/corpus', lambda_=0.1):\n",
    "        \"\"\"Iterates over all whitespace-separated tokens in each file in\n",
    "        `corpus_dir`, and counts the number of occurrences of each unigram and\n",
    "        bigram. Also keeps track of the total number of tokens in the corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus_dir (str): Path to directory containing corpus.\n",
    "            lambda_ (float): Interpolation factor for smoothing by unigram-bigram\n",
    "                interpolation. You only need to save `lambda_` as an attribute for now, and\n",
    "                it will be used later in `LanguageModel.get_bigram_logp`. See Section\n",
    "                IV.1.2. below for further explanation.\n",
    "        \"\"\"\n",
    "        self.lambda_ = lambda_\n",
    "        self.total_num_tokens = 0        # Counts total number of tokens in the corpus\n",
    "        self.unigram_counts = Counter()  # Maps strings w_1 -> count(w_1)\n",
    "        self.bigram_counts = Counter()   # Maps tuples (w_1, w_2) -> count((w_1, w_2))\n",
    "\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have counted the unigrams and bigrams in our corpus, we will add methods for computing query probabilities. First, however, a note about handling bigrams which never occur in our corpus:\n",
    "\n",
    "<a id='smoothing'></a>\n",
    "#### IV.1.2. Smoothing by Interpolation\n",
    "\n",
    "The unigram probability model will also serve as our vocabulary, since we are making the assumption that our query language is derived from our document corpus. As a result, we do not need to perform [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) on our unigram probabilities, since our candidates will be drawn from this very vocabulary. However, even if we have two query terms that are both members of our query language, there is no guarantee that their corresponding *bigram* appears in our training corpus. To handle this data sparsity problem, we will *interpolate* unigram and bigram probabilities to get our ﬁnal conditional probability estimates:\n",
    "$$\n",
    "P(w_2\\mid w_1) = \\lambda P_{\\text{MLE}}(w_2) + (1 - \\lambda)P_{\\text{MLE}}(w_2\\mid w_1).\n",
    "$$\n",
    "Try setting $\\lambda$ to a small value (say, 0.1) in the beginning, and experiment later with varying this parameter to see if you can get better correction accuracies on the development dataset. However, be careful not to overﬁt your development dataset. (You might consider reserving a small portion of your development data to tune the parameters).\n",
    "\n",
    "Fill out the functions below to complete our `LanguageModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/language_model_part2.py\n",
    "\n",
    "# NOTE: Syntax on the following line just extends the `LanguageModel` class\n",
    "class LanguageModel(LanguageModel):\n",
    "    def get_unigram_logp(self, unigram):\n",
    "        \"\"\"Computes the log-probability of `unigram` under this `LanguageModel`.\n",
    "\n",
    "        Args:\n",
    "            unigram (str): Unigram for which to compute the log-probability.\n",
    "\n",
    "        Returns:\n",
    "            log_p (float): Log-probability of `unigram` under this\n",
    "                `LanguageModel`.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def get_bigram_logp(self, w_1, w_2):\n",
    "        \"\"\"Computes the log-probability of `unigram` under this `LanguageModel`.\n",
    "\n",
    "        Note:\n",
    "            Use self.lambda_ for the unigram-bigram interpolation factor.\n",
    "\n",
    "        Args:\n",
    "            w_1 (str): First word in bigram.\n",
    "            w_2 (str): Second word in bigram.\n",
    "\n",
    "        Returns:\n",
    "            log_p (float): Log-probability of `bigram` under this\n",
    "                `LanguageModel`.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def get_query_logp(self, query):\n",
    "        \"\"\"Computes the log-probability of `query` under this `LanguageModel`.\n",
    "\n",
    "        Args:\n",
    "            query (str): Whitespace-delimited sequence of terms in the query.\n",
    "\n",
    "        Returns:\n",
    "            log_p (float): Log-probability assigned to the query under this\n",
    "                `LanguageModel`.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your implementation passes the following sanity checks\n",
    "# Note: Constructing the language model could take 30 seconds or longer\n",
    "# We suggest using `tqdm` to track progress in your `LanguageModel.__init__` function.\n",
    "lm = LanguageModel()\n",
    "\n",
    "assert len(lm.unigram_counts) == 347071, 'Invalid num. unigrams: {}'.format(len(lm.unigram_counts))\n",
    "assert len(lm.bigram_counts) == 4497257, 'Invalid num. bigrams: {}'.format(len(lm.bigram_counts))\n",
    "assert lm.total_num_tokens == 25498340, 'Invalid num. tokens: {}'.format(lm.total_num_tokens)\n",
    "\n",
    "# Test a reasonable query with and without typos (you should try your own)!\n",
    "query_wo_typo = \"stanford university\"\n",
    "query_w_typo = \"stanfrod universit\"\n",
    "\n",
    "p_wo_typo = math.exp(lm.get_query_logp(query_wo_typo))\n",
    "p_w_typo = math.exp(lm.get_query_logp(query_w_typo))\n",
    "print('P(\"{}\") == {}'.format(query_wo_typo, p_wo_typo))\n",
    "print('P(\"{}\") == {}'.format(query_w_typo, p_w_typo))\n",
    "if p_wo_typo <= p_w_typo:\n",
    "    print('Are you sure \"{}\" should be assigned higher probability than \"{}\"?'\n",
    "          .format(query_w_typo, query_wo_typo))\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2. Edit Probability Model\n",
    "\n",
    "The edit probability model attempts to estimate $P(R\\mid Q)$. That is, for a fixed candidate query $Q$, the edit probability model estimates the probability that a (possibly corrupt) raw query $R$ was submitted. We quantify the distance between the candidate query $Q$ and the actual input $R$ using the [Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance). In Damerau-Levenshtein distance, the possible edits are **insertion**, **deletion**, **substitution**, and **transposition**, each involving single characters as operands. We have provided a base class for `EditCostModel`s below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/base_edit_probability_model.py\n",
    "\n",
    "class BaseEditProbabilityModel:\n",
    "    def get_edit_logp(self, edited, original):\n",
    "        \"\"\"Gets the log-probability of editing `original` to arrive at `edited`.\n",
    "        The `original` and `edited` arguments are both single terms that are at\n",
    "        most one edit apart.\n",
    "        \n",
    "        Note: The order of the arguments is chosen so that it reads like an\n",
    "        assignment expression:\n",
    "            > edited := EDIT_FUNCTION(original)\n",
    "        or, alternatively, you can think of it as a (unnormalized) conditional probability:\n",
    "            > log P(edited | original)\n",
    "\n",
    "        Args:\n",
    "            edited (str): Edited term.\n",
    "            original (str): Original term.\n",
    "\n",
    "        Returns:\n",
    "            logp (float): Log-probability of `edited` given `original`\n",
    "                under this `EditProbabilityModel`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError  # Force subclass to implement this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is important to understand that `get_edit_logp` will be called with `original` and `edited` each being single terms that are at most 1 edit apart.** Moreover, its outputs need not be normalized probabilities that sum to 1 over all possible edits to `original` (you can think of the return value more as a \"likelihood score\" than a true probability). We provide an example usage below for clarity:\n",
    "```python\n",
    "epm = EditProbabilityModelSubclass(...)  # You will define such a subclass later\n",
    "original = 'stanford'\n",
    "edited = 'stanfrod'                      # Edited by transposing 'o' and 'r'\n",
    "score = epm.get_edit_logp(edited, original)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.2.1. Uniform-Cost Edit Model\n",
    "\n",
    "As a first pass, we will implement a *uniform-cost edit model.* This model simplifies the computation of the edit probability by assuming that every individual edit in the Damerau-Levenshtein distance has the same probability. You should try a range of values for your uniform edit probability, but in the beginning 0.01 - 0.10 is appropriate. One important thing to remember in building your model is that the user's input query $R$ may indeed be the right one in a majority of cases (*i.e.,* $R = Q$). Thus we typically choose a high ﬁxed probability for `edited == original`; a reasonable range is 0.90 - 0.95.\n",
    "\n",
    "The edit probability model that you construct here will be used when you rank candidates for query corrections. The candidate generator (described in the next section) will make one edit at a time, and it will call the edit probability model each time it makes a single edit to a term, summing log-probabilities for multi-edit changes. Therefore, all you need to do in this part is to calculate the probability of `edited` given that it is **at most one edit from `original`.** This means that `get_edit_logp` will be very simple in this case.\n",
    "\n",
    "Fill out the following class to implement a uniform-cost edit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/uniform_edit_probability_model.py\n",
    "\n",
    "class UniformEditProbabilityModel(BaseEditProbabilityModel):\n",
    "    def __init__(self, edit_prob=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            edit_prob (float): Probability of a single edit occurring, where\n",
    "                an edit is an insertion, deletion, substitution, or transposition,\n",
    "                as defined by the Damerau-Levenshtein distance.\n",
    "        \"\"\"\n",
    "        self.edit_prob = edit_prob\n",
    "\n",
    "    def get_edit_logp(self, edited, original):\n",
    "        \"\"\"Gets the log-probability of editing `original` to arrive at `edited`.\n",
    "        The `original` and `edited` arguments are both single terms that are at\n",
    "        most one edit apart.\n",
    "        \n",
    "        Note: The order of the arguments is chosen so that it reads like an\n",
    "        assignment expression:\n",
    "            > edited := EDIT_FUNCTION(original)\n",
    "        or, alternatively, you can think of it as a (unnormalized) conditional probability:\n",
    "            > log P(edited | original)\n",
    "\n",
    "        Args:\n",
    "            edited (str): Edited term.\n",
    "            original (str): Original term.\n",
    "\n",
    "        Returns:\n",
    "            logp (float): Log-probability of `edited` given `original`\n",
    "                under this `EditProbabilityModel`.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you pass the following sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDIT_PROB = 0.05\n",
    "epm = UniformEditProbabilityModel(edit_prob=EDIT_PROB)\n",
    "\n",
    "# Test a basic edit\n",
    "edited, original = 'stanfrod', 'stanford'\n",
    "assert math.isclose(epm.get_edit_logp(edited, original), math.log(EDIT_PROB))\n",
    "\n",
    "# Test a non-edit\n",
    "assert math.isclose(epm.get_edit_logp(original, original), math.log(1. - EDIT_PROB))\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3. Candidate Generator\n",
    "\n",
    "Recall that the candidate generator takes a raw query $R$ submitted by the user, and generates candidates for the intended query $Q$. Since we know that more than 97% of spelling errors are found within an edit distance of 2 from the user's intended query, we encourage you to consider possible query corrections that are within distance 2 of $R$. This is the approach taken by Peter Norvig in [his essay on spelling correction](http://norvig.com/spell-correct.html). However, it is not tractable to use a pure \"brute force\" generator that produces all possible strings within distance 2 of $R$, because for any $R$ of non-trivial length, the number of candidates would be enormous. Thus we would have to evaluate the language and edit probability models on a huge number of candidates.\n",
    "\n",
    "\n",
    "#### IV.3.1. Candidate Generator with Restricted Search Space\n",
    "\n",
    "We can make the naïve approach tractable by aggressively narrowing down the search space while generating candidates. There are many valid approaches to efficient candidate generation, but here are a few basic ideas:\n",
    "  - Begin by looking at *each individual term* in the query string $R$, and consider all possible edits that are distance 1 from that term.\n",
    "  - Remember that you might consider hyphens and/or spaces as elements of your character set. This will allow you to consider some relatively common errors, like when a space is accidentally inserted in a word, or two terms in the query were mistakenly separated by a space when they should actually be joined.\n",
    "  - Each time you generate an edit to a term, make sure that the edited term appears in the dictionary. (Remember that we have assumed that all words in a valid candidate query will be found in our training corpus, as mentioned above in [Section IV.1.2](#smoothing) above).\n",
    "  - If you have generated possible edits to multiple individual terms, take the Cartesian product over these terms to produce a complete candidate query that includes edits to multiple terms. (But remember that you probably shouldn't go beyond a total edit distance of 2 for the query overall).\n",
    "  \n",
    "Again, there are many possible extensions and variations on the strategies mentioned here. We encourage you to explore some diﬀerent options, and then describe in your written report the strategies that you ultimately used, and how you optimized their performance. Note that **solutions that exhaustively generate and score all possible query candidates at edit distances 1 and 2 will run too slowly and will not receive full credit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/candidate_generator.py\n",
    "\n",
    "class CandidateGenerator:\n",
    "    # Alphabet to use for insertion and substitution\n",
    "    alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "                'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "                '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                ' ', ',', '.', '-']\n",
    "\n",
    "    def __init__(self, lm, epm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lm (LanguageModel): Language model to use for prior probabilities, P(Q).\n",
    "            epm (EditProbabilityModel): Edit probability model to use for P(R|Q).\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.epm = epm\n",
    "\n",
    "    def get_num_oov(self, query):\n",
    "        \"\"\"Get the number of out-of-vocabulary (OOV) words in `query`.\"\"\"\n",
    "        return sum(1 for w in query.strip().split()\n",
    "                   if w not in self.lm.unigram_counts)\n",
    "\n",
    "    def filter_and_yield(self, query, lp):\n",
    "        if query.strip() and self.get_num_oov(query) == 0:\n",
    "            yield query, lp\n",
    "\n",
    "    def get_candidates(self, query):\n",
    "        \"\"\"Starts from `query`, and performs EDITS OF DISTANCE <=2 to get new\n",
    "        candidate queries. To make scoring tractable, only returns/yields\n",
    "        candidates that satisfy certain criteria (ideas for such criteria are\n",
    "        described in bullet points above).\n",
    "\n",
    "        Hint: We suggest you implement a helper function that takes a term and\n",
    "            generates all possible edits of distance one from that term.\n",
    "            It should probably only return edits that are in the vocabulary\n",
    "            (i.e., edits for which `self.get_num_oov(edited) == 0`).\n",
    "\n",
    "        Args:\n",
    "            query (str): Starting query.\n",
    "\n",
    "        Returns:\n",
    "            Iterable over tuples (cdt, cdt_edit_logp) of candidates and\n",
    "                their associated edit log-probabilities. Return value could be\n",
    "                a list or a generator yielding tuples of this form.\n",
    "        \"\"\"\n",
    "        # Yield the unedited query first\n",
    "        # We provide this line as an example of how to use `self.filter_and_yield`\n",
    "        yield from self.filter_and_yield(query, self.epm.get_edit_logp(query, query))\n",
    "\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your candidate generator passes the following sanity checks. Feel free to add more tests here as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = CandidateGenerator(lm, epm)\n",
    "query = 'stanford university'\n",
    "num_candidates = 0\n",
    "did_generate_original = False\n",
    "for candidate, candidate_logp in cg.get_candidates(query):\n",
    "    num_candidates += 1\n",
    "    if candidate == query:\n",
    "        did_generate_original = True\n",
    "\n",
    "    assert cg.get_num_oov(query) == 0, \\\n",
    "        \"You should not generate queries with out-of-vocab terms ('{}' has OOV terms)\".format(candidate)\n",
    "\n",
    "assert 1e2 <= num_candidates <= 1e4, \\\n",
    "    \"You should generate between 100 and 10,000 terms (generated {})\".format(num_candidates)\n",
    "\n",
    "assert did_generate_original, \"You should generate the original query ({})\".format(query)\n",
    "\n",
    "### Begin your code\n",
    "\n",
    "### End your code\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.4. Candidate Scorer\n",
    "\n",
    "The candidate scorer's job is to find the most likely query $Q$ given the raw query $R$. It does this by combining the language model for $P(Q)$, the edit probability model for $P(R\\mid Q)$, and the candidate generator (to get candidates for $Q$). Formally, given raw query $R$, the candidate scorer outputs\n",
    "$$\n",
    "    Q^{*} = \\arg\\max_{Q_{i}} P(Q_{i}\\mid R) = \\arg\\max_{Q_{i}} P(R\\mid Q_{i}) P(Q_{i}),\n",
    "$$\n",
    "where the max is taken over candidate queries $Q_{i}\\in\\{Q_1, \\ldots, Q_{n}\\}$ produced by the candidate generator given $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.4.1. Candidate Scorer with Weighting\n",
    "When combining probabilities from the language model and the edit probability model, we can use a parameter to weight the two models differently:\n",
    "$$\n",
    "    P(Q\\mid R)\\propto P(R\\mid Q)P(Q)^{\\mu}.\n",
    "$$\n",
    "Start out with $\\mu = 1$, and then experiment later with different values of $\\mu$ to see which one gives you the best spelling correction accuracy. Again, be careful not to overfit your development dataset. \n",
    "\n",
    "Fill out the following class to complete the spelling corrector with uniform edit cost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/candidate_scorer.py\n",
    "\n",
    "class CandidateScorer:\n",
    "    \"\"\"Combines the `LanguageModel`, `EditProbabilityModel`, and\n",
    "    `CandidateGenerator` to produce the most likely query Q given a raw query R.\n",
    "    Since the candidate generator already uses the edit probability model, we\n",
    "    do not need to take the edit probability model as an argument in the constructor.\n",
    "    \"\"\"\n",
    "    def __init__(self, lm, cg, mu=1.):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lm (LanguageModel): Language model for estimating P(Q).\n",
    "            cg (CandidateGenerator): Candidate generator for generating possible Q.\n",
    "            mu (float): Weighting factor for the language model (see write-up).\n",
    "                Remember that our probability computations are done in log-space.\n",
    "        \"\"\"\n",
    "        self.lm = lm\n",
    "        self.cg = cg\n",
    "        self.mu = mu\n",
    "\n",
    "    def get_score(self, query, log_edit_prob):\n",
    "        \"\"\"Uses the language model and `log_edit_prob` to compute the final\n",
    "        score for a candidate `query`. Uses `mu` as weighting exponent for P(Q).\n",
    "\n",
    "        Args:\n",
    "            query (str): Candidate query.\n",
    "            log_edit_prob (float): Log-probability of candidate query given\n",
    "                original query (i.e., log(P(R|Q), where R is `query`).\n",
    "\n",
    "        Returns:\n",
    "            log_p (float): Final score for the query, i.e., the log-probability\n",
    "                of the query.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def correct_spelling(self, r):\n",
    "        \"\"\"Corrects spelling of raw query `r` to get the intended query `q`.\n",
    "\n",
    "        Args:\n",
    "            r (str): Raw input query from the user.\n",
    "\n",
    "        Returns:\n",
    "            q (str): Spell-corrected query. That is, the query that maximizes\n",
    "                P(R|Q)*P(Q) under the language model and edit probability model,\n",
    "                restricted to Q's generated by the candidate generator.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes LanguageModel lm was already built above\n",
    "print('Building edit probability model...')\n",
    "epm = UniformEditProbabilityModel()\n",
    "print('Building candidate generator...')\n",
    "cg = CandidateGenerator(lm, epm)\n",
    "print('Building candidate scorer model...')\n",
    "cs = CandidateScorer(lm, cg, mu=1.0)\n",
    "print('Running spelling corrector...')\n",
    "\n",
    "# Add your own queries here to test your spelling corrector\n",
    "queries = [('stanfrod university', 'stanford university'),\n",
    "           ('stanford unviersity', 'stanford university'),\n",
    "           ('sanford university', 'stanford university')]\n",
    "for query, expected in queries:\n",
    "    corrected = cs.correct_spelling(query)\n",
    "    print(\"\\t'{}' corrected to '{}'\".format(query, corrected))\n",
    "    assert corrected == expected, \"Expected '{}', got '{}'\".format(expected, corrected)\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.4.2. Dev Set Evaluation (Uniform)\n",
    "\n",
    "Now that we have constructed a basic spelling corrector, we will evaluate its performance on the held-out dev set. Recall that the dev set is stored across the files in `pa2-data/dev_set/`:\n",
    "  - `queries.txt`: One raw query $R$ per line.\n",
    "  - `google.txt`: Google's corrected queries $Q$ (one per line, same order as `queries.txt`).\n",
    "  - `gold.txt`: Ground-truth queries $Q$ (again, one per line, same order).\n",
    "  \n",
    "Run the following cells to evaluate your spelling corrector on the dev set using your uniform edit probability model. We will also evaluate your model on a private test set after submission. For full credit, your spelling corrector with uniform edit probability model should achieve accuracy within 1% of the staff implementation *on the test set.* **We do not provide test set queries, but as a guideline for performance, the staff implementation gets 82.42% accuracy on the dev set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_eval(candidate_scorer, verbose=False):\n",
    "    \"\"\"Evaluate `candidate_scorer` on the dev set.\"\"\"\n",
    "    query_num = 1\n",
    "    yours_correct = 0\n",
    "    google_correct = 0\n",
    "    # Read originals, ground-truths, Google's predictions\n",
    "    dev_dir = 'pa2-data/dev_set/'\n",
    "    with tqdm(total=455, unit=' queries') as pbar, \\\n",
    "            open(os.path.join(dev_dir, 'queries.txt'), 'r') as query_fh, \\\n",
    "            open(os.path.join(dev_dir, 'gold.txt'), 'r') as gold_fh, \\\n",
    "            open(os.path.join(dev_dir, 'google.txt'), 'r') as google_fh:\n",
    "        while True:\n",
    "            # Read one line\n",
    "            query = query_fh.readline().rstrip('\\n')\n",
    "            if not query:\n",
    "                # Finished all queries\n",
    "                break\n",
    "            corrected = candidate_scorer.correct_spelling(query)\n",
    "            corrected = ' '.join(corrected.split())  # Squash multiple spaces\n",
    "            gold = gold_fh.readline().rstrip('\\n')\n",
    "            google = google_fh.readline().rstrip('\\n')\n",
    "\n",
    "            # Count whether correct\n",
    "            if corrected == gold:\n",
    "                yours_correct += 1\n",
    "            if google == gold:\n",
    "                google_correct += 1\n",
    "\n",
    "            # Print running stats\n",
    "            yours_accuracy = yours_correct / query_num * 100\n",
    "            google_accuracy = google_correct / query_num * 100\n",
    "            if verbose:\n",
    "                print('QUERY {:03d}'.format(query_num))\n",
    "                print('---------')\n",
    "                print('(original):      {}'.format(query))\n",
    "                print('(corrected):     {}'.format(corrected))\n",
    "                print('(google):        {}'.format(google))\n",
    "                print('(gold):          {}'.format(gold))\n",
    "                print('Google accuracy: {}/{} ({:5.2f}%)\\n'\n",
    "                      .format(google_correct, query_num, google_accuracy))\n",
    "                print('Your accuracy:   {}/{} ({:5.2f}%)'\n",
    "                      .format(yours_correct, query_num, yours_accuracy))\n",
    "            \n",
    "            pbar.set_postfix(google='{:5.2f}%'.format(google_accuracy),\n",
    "                             yours='{:5.2f}%'.format(yours_accuracy))\n",
    "            pbar.update()\n",
    "            query_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set verbose=True for debugging output\n",
    "# For reference, our implementation takes ~1 min, 40 sec to run and gets 82.42% accuracy\n",
    "dev_eval(cs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='empirical'></a>\n",
    "## V. Task 2: Spelling Correction with Empirical Edit Costs (25%)\n",
    "\n",
    "\n",
    "### V.1. Improved Edit Probability Model\n",
    "\n",
    "Now that our spelling corrector is working correctly with a basic edit probability model, we will turn our attention to a somewhat more realistic approach to edit probabilities. In this task, we will learn these edit probabilities from the empirical error data provided in `data/training_set/edit1s.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1.1. Empirical Edit Costs\n",
    "\n",
    "As outlined in [Section III](#dataset) above, you have been given a list of query pairs that are precisely edit distance 1 from each other. The ﬁrst step for this task is to devise a simple algorithm to determine which speciﬁc edit exists between the two queries in each pair. By aggregating the counts of all such edits over all queries, you can estimate the probability of each individual edit. The edit probability calculation is described in more detail in the [lecture handout on spelling correction](http://web.stanford.edu/class/cs276/handouts/spell_correction.pdf). As an example, if you need to determine the probability of the letter 'e' being (mistakenly) replaced by the letter 'a' in a query, you should calculate:\n",
    "$$\n",
    "    P(\\texttt{sub}[a, e]) = \\frac{\\texttt{count}(\\texttt{sub}[a, e])}{\\texttt{count}(e)}.\n",
    "$$\n",
    "Note that the insertion and deletion operator probabilities are conditioned on the character before the character being operated on &mdash; which also means that you should devise an appropriate solution to handle the special case of insertions or deletions occurring at the beginning of a word. Finally, to account for the inevitable problem of data sparsity in our edit training ﬁle, you should apply Laplace add-one smoothing to the edit probabilities, as described in the lecture handout (linked above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/empirical_edit_probability_model.py\n",
    "\n",
    "class Edit:\n",
    "    \"\"\"Represents a single edit in Damerau-Levenshtein distance.\n",
    "    We use this class to count occurrences of different edits in the training data.\n",
    "    \"\"\"\n",
    "    INSERTION = 1\n",
    "    DELETION = 2\n",
    "    TRANSPOSITION = 3\n",
    "    SUBSTITUTION = 4\n",
    "\n",
    "    def __init__(self, edit_type, c1=None, c2=None):\n",
    "        \"\"\"\n",
    "        Members:\n",
    "            edit_type (int): One of Edit.{NO_EDIT,INSERTION,DELETION,\n",
    "                TRANSPOSITION,SUBSTITUTION}.\n",
    "            c1 (str): First (in original) char involved in the edit.\n",
    "            c2 (str): Second (in original) char involved in the edit.\n",
    "        \"\"\"\n",
    "        self.edit_type = edit_type\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "\n",
    "class EmpiricalEditProbabilityModel(BaseEditProbabilityModel):\n",
    "\n",
    "    START_CHAR = ''      # Used to indicate start-of-query\n",
    "    NO_EDIT_PROB = 0.92  # Hyperparameter for probability assigned to no-edit\n",
    "\n",
    "    def __init__(self, training_set_path='pa2-data/training_set/edit1s.txt'):\n",
    "        \"\"\"Builds the necessary data structures to compute log-probabilities of\n",
    "        distance-1 edits in constant time. In particular, counts the unigrams\n",
    "        (single characters), bigrams (of 2 characters), alphabet size, and\n",
    "        edit count for insertions, deletions, substitutions, and transpositions.\n",
    "\n",
    "        Hint: Use the `Edit` class above. It may be easier to write the `get_edit`\n",
    "        function first, since you should call that function here.\n",
    "\n",
    "        Note: We suggest using tqdm with the size of the training set (819722) to track\n",
    "        the initializers progress when parsing the training set file.\n",
    "\n",
    "        Args:\n",
    "            training_set_path (str): Path to training set of empirical error data.\n",
    "        \"\"\"\n",
    "        # Your code needs to initialize all four of these data structures\n",
    "        self.unigram_counts = Counter()  # Maps chars c1 -> count(c1)\n",
    "        self.bigram_counts = Counter()   # Maps tuples (c1, c2) -> count((c1, c2))\n",
    "        self.alphabet_size = 0           # Counts all possible characters\n",
    "\n",
    "        # Maps edit-types -> dict mapping tuples (c1, c2) -> count(edit[c1, c2])\n",
    "        # Example usage: \n",
    "        #   > e = Edit(Edit.SUBSTITUTION, 'a', 'b')\n",
    "        #   > edit_count = self.edit_counts[e.edit_type][(e.c1, e.c2)]\n",
    "        self.edit_counts = {edit_type: Counter()\n",
    "                            for edit_type in (Edit.INSERTION, Edit.DELETION,\n",
    "                                              Edit.SUBSTITUTION, Edit.TRANSPOSITION)}\n",
    "\n",
    "        with open(training_set_path, 'r') as training_set:\n",
    "            for example in tqdm(training_set, total=819722):\n",
    "                edited, original = example.strip().split('\\t')\n",
    "\n",
    "                ### Begin your code\n",
    "\n",
    "                ### End your code\n",
    "\n",
    "    def get_edit(self, edited, original):\n",
    "        \"\"\"Gets an `Edit` object describing the type of edit performed on `original`\n",
    "        to produce `edited`.\n",
    "\n",
    "        Note: Only edits with an edit distance of at most 1 are valid inputs.\n",
    "\n",
    "        Args:\n",
    "            edited (str): Raw query, which contains exactly one edit from `original`.\n",
    "            original (str): True query. Want to find the edit which turns this into `edited`.\n",
    "\n",
    "        Returns:\n",
    "            edit (Edit): `Edit` object representing the edit to apply to `original` to get `edited`.\n",
    "                If `edited == original`, returns None.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def get_edit_logp(self, edited, original):\n",
    "        \"\"\"Gets the log-probability of editing `original` to arrive at `edited`.\n",
    "        The `original` and `edited` arguments are both single terms that are at\n",
    "        most one edit apart.\n",
    "        \n",
    "        Note: The order of the arguments is chosen so that it reads like an\n",
    "        assignment expression:\n",
    "            > edited := EDIT_FUNCTION(original)\n",
    "        or, alternatively, you can think of it as a (unnormalized) conditional probability:\n",
    "            > log P(edited | original)\n",
    "\n",
    "        Args:\n",
    "            edited (str): Edited term.\n",
    "            original (str): Original term.\n",
    "\n",
    "        Returns:\n",
    "            logp (float): Log-probability of `edited` given `original`\n",
    "                under this `EditProbabilityModel`.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to evaluate your spelling corrector on the dev set using your empirical edit probability model. We will also evaluate your model on a private test set after submission. For full credit, your spelling corrector with uniform edit probability model should achieve accuracy within 1% of the staff implementation *on the test set.* **We do not provide test set queries, but as a guideline for performance, the staff implementation gets 87.91% accuracy on the dev set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build spelling corrector for evaluation on the dev set\n",
    "# For reference, our initialization times are 25 sec for lm, and 1 min, 40 sec for epm\n",
    "lm = LanguageModel()\n",
    "epm = EmpiricalEditProbabilityModel()\n",
    "cg = CandidateGenerator(lm, epm)\n",
    "cs = CandidateScorer(lm, cg, mu=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set verbose=True for debugging output\n",
    "# For reference our implementation takes ~2 min, 30 sec to run and gets 87.91% accuracy\n",
    "dev_eval(cs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='written'></a>\n",
    "## VI. Written Report (20%)\n",
    "\n",
    "Be sure to document any design decisions you made, and give some brief rationale for them. Please keep your report concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI.1. Overall System Design (5%)\n",
    "\n",
    "Provide a concise (at most 5 sentences) description of the overall system design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI.2. Smoothing and Related Techniques (5%)\n",
    "\n",
    "Give a short analysis of smoothing techniques used in this assignment. For example, you might produce a plot comparing different values for $\\lambda$ in unigram-bigram interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI.3. Optimizations for Candidate Generation (5%)\n",
    "\n",
    "Provide a brief description of the techniques you used for optimizing candidate generation. Be sure to include an analysis of the amount by which each optimization sped up the overall spelling correction system, as well as any changes in accuracy you were able to measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI.4. Tuning Parameters (5%)\n",
    "Provide at least two plots showing how accuracy varies as you change parameter values (*e.g.,* $\\mu$ and $\\lambda$). Comment briefly (1-2 sentences) on each plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extra'></a>\n",
    "## VII. Extra Credit (Optional, up to 10%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have listed a few ideas here, but really any extensions that go above and beyond the scope of tasks 1 and 2 will be considered.\n",
    "\n",
    "1. **Expanded edit model.** We saw (or will see) in lecture that there are sometimes spelling errors that may not be within a \"naive\" edit distance 2 of the correct phrase, but that may have a conceptual basis that makes them very common and understandable. (Substituting 'ph' for 'f', or vice versa, is one such example.) Can you incorporate these types of errors into the edit probabilities of your edit probability model?\n",
    "2. **Empirical edit costs using Wikipedia.** In task 2, you used the dataset of queries 1 edit distance apart to learn edit probabilities. If you look at the queries in this dataset, you will observe that most of these queries are related to the Stanford corpus, the same corpus used to build the language model. It would be interesting to explore what happens if the channel model and language model are learned from diﬀerent datasets (and hence diﬀerent distributions of the underlying data). To this end, you can use a dataset of spelling errors collected from Wikipedia and available on Peter Norvig’s website (http://norvig.com/ngrams/spell-errors.txt).\n",
    "3. **Alternate Smoothing.** Try other smoothing algorithms (such as Kneser-Ney smoothing) to better capture probabilities in the training corpus.\n",
    "4. **K-gram index.** To deal with unseen words, it is possible to develop a measure for the probability of that word being spelled correctly by developing a character k-gram index over your corpus. For example, a q not followed by a u should lead to a low probability. This index can also help you generate candidate corrections much more eﬃciently.\n",
    "5. **Levenshtein Automata.** You can do even faster candidate generation using a Levenshtein transducer (http://en.wikipedia.org/wiki/Levenshtein_transducer), which uses a ﬁnite state automata for fuzzy matching of words. There is an experimental implementation in Python at https://gist.github.com/491973, but it needs to be generalized to perform the transposition operation too. This tutorial might be helpful: http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata.\n",
    "\n",
    "Finally, we will give a small amount of extra credit to the best spell correction systems, measured in terms of both accuracy and running time (as computed on our hidden test data). The top 5 systems according to either metric will receive 5% each, while the next 15 systems will receive 2.5% each.\n",
    "\n",
    "**If you decide to tackle an extra credit option, give a brief description of your approach and results below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > Your Answer Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
