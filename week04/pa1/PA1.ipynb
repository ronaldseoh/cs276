{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this programming assignment, you will be applying the knowledge that you have learned in the lectures to build a simple indexing and retrieval system for a search engine. This assignment should be done in teams of two or individually. Assignments are graded the same for one and two person teams. \n",
    "\n",
    "More specifically, it involves the following tasks:\n",
    "1. [Build an uncompressed index (45%)](#Building-an-uncompressed-index-and-retrieval-(45%)) over a corpus of webpages from the stanford.edu domain, and implement retrieval for Boolean conjunctive queries.\n",
    "2. [Build a compressed index (30%)](#Building-a-compressed-index-(30%)) over the same corpus using variable length encoding and implement Boolean conjunctive queries.\n",
    "3. [Write up a report (25%)](#Report-(25%)) describing your program and answer a set of questions.\n",
    "4. [For extra credit (10%)](#Additional-compression-methods-(10%---Extra-credit)) you are encouraged to implement other compression algorithms (e.g., gamma-encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Submission instructions\n",
    "\n",
    "1\\. The assignment is due before class at 4:00 pm on the due date (23rd April 2019)\n",
    "\n",
    "2\\. The notebook will automatically generate **python files** in submission folder. You'll have to upload them to the PA1-code assignment on gradescope. Note that you need to upload all the individual files in the submission folder without zipping it.    \n",
    "\n",
    "3\\. While solving the assignment, do **NOT** change class and method names, autograder tests will fail otherwise. \n",
    "\n",
    "4\\. You'll also have to upload a **PDF version** of the notebook (which would be primarily used to grade your report section of the notebook) to PA1-PDF assignment on gradescope. Note that directly converting the PDF truncates code cells. To get a usable PDF version, first click on File > Print Preview, which will open in a new tab, then print to PDF using your browser's print functionality. \n",
    "\n",
    "5\\. After uploading the PDF make sure you **tag all the relevant pages to each question**. We will penalize for mistagged submissions. \n",
    "\n",
    "6\\. If you are solving the assignment in a team of two, add the other student as a group member after submitting the assignment. Do **NOT** submit the same assignment twice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the tee magic which saves a copy of the cell when executed\n",
    "%reload_ext autograding_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `submission` folder will contain all the files to be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try: \n",
    "    os.mkdir('submission')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/imports.py\n",
    "\n",
    "# You can add additional imports here\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import array\n",
    "import os\n",
    "import timeit\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus you will be working with for this assignment contains webpages from the stanford.edu domain. The data for this assignment is available as a .zip file at: http://web.stanford.edu/class/cs276/pa/pa1-data.zip. The following code puts the data folder under the current directory. The total size of the corpus is about 170MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "data_url = 'http://web.stanford.edu/class/cs276/pa/pa1-data.zip'\n",
    "data_dir = 'pa1-data'\n",
    "urllib.request.urlretrieve(data_url, data_dir+'.zip')\n",
    "zip_ref = zipfile.ZipFile(data_dir+'.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index will be stored in `output_dir` and `tmp` will contain some temporary files for toy indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('tmp')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "try: \n",
    "    os.mkdir('toy_output_dir')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 sub-directories (named 0-9) under the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir('pa1-data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file under the sub-directory is the content of an individual webpage. You should assume that the individual file names are unique within each sub-directory, but not necessarily the case across sub-directories (*i.e.,* the full path of the files are unique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir('pa1-data/0'))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the HTML info has been stripped off from those webpages, so they only contain space-delimited words. You should not re-tokenize the words. Each consecutive span of non-space characters consists of a word token in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pa1-data/0/3dradiology.stanford.edu_', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also find a small corpus included with the assignment in the folder `toy-data`. We will use this toy dataset to test our code before running on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dir = 'toy-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an uncompressed index and retrieval (45%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment is to build an inverted index of this corpus, and implement Boolean conjunctive queries. **In particular, you need to implement the blocked sort-based indexing (BSBI) algorithm described in [Section 4.2](http://nlp.stanford.edu/IR-book/pdf/04const.pdf) of the textbook.** While we quote some fundamentals from the textbook, we strongly suggest that you read Section 4.2 in more detail to fully understand the BSBI algorithm.\n",
    "\n",
    "> To construct an index, we first make a pass through the collection assembling all term-docID pairs. We then sort the pairs with the term as the dominant key and docID as the secondary key. Finally, we organize the docIDs for each term into a postings list and compute statistics like term and document frequency. For small collections, all this can be done in memory. \n",
    "\n",
    "In this assignment, you will be implementing methods for large collections that require the use of secondary storage (*i.e.,* disk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IdMap\n",
    "\n",
    "Again quoting from Section 4.2:\n",
    "\n",
    "> To make index construction more efficient, we represent terms as termIDs (instead of strings), where each termID is a unique serial number. We can build the mapping from terms to termIDs on the fly while we are processing the collection. Similarly, we also represent documents as docIDs (instead of strings).\n",
    "\n",
    "Let us first build a helper class `IdMap`, which maps strings to numeric ids (and vice-versa). We will need this to bijectively map term to termID and doc to docID.\n",
    "\n",
    "Fill in the functions `_get_str` and `_get_id` in the following code. The only interface to this class is provided by `__getitem__` which gets the correct mapping depending on the type of the key. (See [these docs](https://docs.python.org/3.7/reference/datamodel.html#special-method-names) if you are unfamiliar with special functions like `__getitem__` ). Further, to simplify code later on, we'll also add some logic to add a string if it doesn't already exist in the map.   \n",
    "\n",
    "We will use a dictionary for efficient access from strings to numeric ids, and we will use a list for efficient access (and storage) from numeric ids to strings.\n",
    "\n",
    "You might also find this [short tutorial](https://www.omkarpathak.in/2018/04/11/python-getitem-and-setitem/) useful to get started with special (a.k.a magic) methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/id_map.py\n",
    "class IdMap:\n",
    "    \"\"\"Helper class to store a mapping from strings to ids.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return number of terms stored in the IdMap\"\"\"\n",
    "        return len(self.id_to_str)\n",
    "        \n",
    "    def _get_str(self, i):\n",
    "        \"\"\"Returns the string corresponding to a given id (`i`).\"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "        \n",
    "    def _get_id(self, s):\n",
    "        \"\"\"Returns the id corresponding to a string (`s`). \n",
    "        If `s` is not in the IdMap yet, then assigns a new id and returns the new id.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "            \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"If `key` is a integer, use _get_str; \n",
    "           If `key` is a string, use _get_id;\"\"\"\n",
    "        if type(key) is int:\n",
    "            return self._get_str(key)\n",
    "        elif type(key) is str:\n",
    "            return self._get_id(key)\n",
    "        else:\n",
    "            raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it passes the following simple test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIdMap = IdMap()\n",
    "assert testIdMap['a'] == 0, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['bcd'] == 1, \"Unable to add a new string to the IdMap\"\n",
    "assert testIdMap['a'] == 0, \"Unable to retrieve the id of an existing string\"\n",
    "assert testIdMap[1] == 'bcd', \"Unable to retrive the string corresponding to a\\\n",
    "                                given id\"\n",
    "try:\n",
    "    testIdMap[2]\n",
    "except IndexError as e:\n",
    "    assert True, \"Doesn't throw an IndexError for out of range numeric ids\"\n",
    "assert len(testIdMap) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward we will expect you to write your own test cases to make sure parts of your program are working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Postings List as bytearrays\n",
    "\n",
    "In order to write and read lists of postings (docIDs) efficiently from the disk, we store them as bytearrays. We've provided an implementation of `UncompressedPostings` class which contains static encode and decode functions. In the next task you'll be required to implement compressed versions with the same inferface. \n",
    "\n",
    "For reference:\n",
    "1. https://docs.python.org/3/library/array.html\n",
    "2. https://pymotw.com/3/array/#module-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncompressedPostings:\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes postings_list into a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            List of docIDs (postings)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bytes\n",
    "            bytearray representing integers in the postings_list\n",
    "        \"\"\"\n",
    "        return array.array('L', postings_list).tobytes()\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes postings_list from a stream of bytes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            bytearray representing encoded postings list as output by encode \n",
    "            function\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded list of docIDs from encoded_postings_list\n",
    "        \"\"\"\n",
    "        \n",
    "        decoded_postings_list = array.array('L')\n",
    "        decoded_postings_list.frombytes(encoded_postings_list)\n",
    "        return decoded_postings_list.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how it works, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = UncompressedPostings.encode([1,2,3])\n",
    "print(x)\n",
    "print(UncompressedPostings.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index on Disk\n",
    "\n",
    "> With main memory insufficient, we need to use an external sorting algorithm, that is, one that uses disk. For acceptable speed, the central requirement of such an algorithm is that it minimize the number of random disk seeks during sorting - sequential disk reads are far faster than seeks. \n",
    "\n",
    "In this section we provide a base class `InvertedIndex` which would be subsequently subclassed into `InvertedIndexWriter`, `InvertedIndexIterator` and `InvertedIndexMapper`. While typically `cPickle` is used for serialization in Python, it does not support partial reads and writes, which is why we are rolling out our own custom solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"A class that implements efficient reads and writes of an inverted index \n",
    "    to disk\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    postings_dict: Dictionary mapping: termID->(start_position_in_index_file, \n",
    "                                                number_of_postings_in_list,\n",
    "                                               length_in_bytes_of_postings_list)\n",
    "        This is a dictionary that maps from termIDs to a 3-tuple of metadata \n",
    "        that is helpful in reading and writing the postings in the index file \n",
    "        to/from disk. This mapping is supposed to be kept in memory. \n",
    "        start_position_in_index_file is the position (in bytes) of the postings \n",
    "        list in the index file\n",
    "        number_of_postings_in_list is the number of postings (docIDs) in the \n",
    "        postings list\n",
    "        length_in_bytes_of_postings_list is the length of the byte \n",
    "        encoding of the postings list\n",
    "    \n",
    "    terms: List[int]\n",
    "        A list of termIDs to remember the order in which terms and their \n",
    "        postings lists were added to index. \n",
    "        \n",
    "        After Python 3.7 we technically no longer need it because a Python dict \n",
    "        is an OrderedDict, but since it is a relatively new feature, we still\n",
    "        maintain backward compatibility with a list to keep track of order of \n",
    "        insertion. \n",
    "    \"\"\"\n",
    "    def __init__(self, index_name, postings_encoding=None, directory=''):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        index_name (str): Name used to store files related to the index\n",
    "        postings_encoding: A class implementing static methods for encoding and \n",
    "            decoding lists of integers. Default is None, which gets replaced\n",
    "            with UncompressedPostings\n",
    "        directory (str): Directory where the index files will be stored\n",
    "        \"\"\"\n",
    "\n",
    "        self.index_file_path = os.path.join(directory, index_name+'.index')\n",
    "        self.metadata_file_path = os.path.join(directory, index_name+'.dict')\n",
    "\n",
    "        if postings_encoding is None:\n",
    "            self.postings_encoding = UncompressedPostings\n",
    "        else:\n",
    "            self.postings_encoding = postings_encoding\n",
    "        self.directory = directory\n",
    "\n",
    "        self.postings_dict = {}\n",
    "        self.terms = []         #Need to keep track of the order in which the \n",
    "                                #terms were inserted. Would be unnecessary \n",
    "                                #from Python 3.7 onwards\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Opens the index_file and loads metadata upon entering the context\"\"\"\n",
    "        # Open the index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "\n",
    "        # Load the postings dict and terms from the metadata file\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.terms = pkl.load(f)\n",
    "            self.term_iter = self.terms.__iter__()                       \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Closes the index_file and saves metadata upon exiting the context\"\"\"\n",
    "        # Close the index file\n",
    "        self.index_file.close()\n",
    "        \n",
    "        # Write the postings dict and terms to the metadata file\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interacting with a file on disk, we provide `__enter__` and `__exit__` functions, which allow using the `with` statement just like you would with file IO in python (Refer to the [official documentation for context managers](https://docs.python.org/3/library/contextlib.html)).\n",
    "\n",
    "Here is an example of how to use the `InvertedIndexWriter` context manager:\n",
    "\n",
    "```python\n",
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    # Some code here\n",
    "```\n",
    "\n",
    "If you are unfamiliar with context managers in python, we would recommend checking out these tutorials ([1](https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/), [2](http://arnavk.com/posts/python-context-managers/)), although it isn't necessary to be able to understand all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "> BSBI (i) segments the collection into parts of equal size, (ii) sorts the termID-docID pairs of each part in memory, (iii) stores intermediate sorted results on disk, and (iv) merges all intermediate results into the final index. \n",
    "\n",
    "You should treat each sub-directory as a block, and only load one block in memory at a time when you build your index (note: you will be penalized if you construct the index by loading blocks of larger than one directory in memory at once). Note that we are abstracting away from the operating systems meaning of blocks. You can assume that each block is small enough to be stored in memory.\n",
    "\n",
    "In this section, we introduce the class `BSBIIndex` which we will progressively build. The `index` function provides the skeleton for BSBI and your job is to implement `parse_block`, `invert_write` and `merge` functions in subsequent sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do not make any changes here, they will be overwritten while grading\n",
    "class BSBIIndex:\n",
    "    \"\"\" \n",
    "    Attributes\n",
    "    ----------\n",
    "    term_id_map(IdMap): For mapping terms to termIDs\n",
    "    doc_id_map(IdMap): For mapping relative paths of documents (eg \n",
    "        0/3dradiology.stanford.edu_) to docIDs\n",
    "    data_dir(str): Path to data\n",
    "    output_dir(str): Path to output index files\n",
    "    index_name(str): Name assigned to index\n",
    "    postings_encoding: Encoding used for storing the postings.\n",
    "        The default (None) implies UncompressedPostings\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir, index_name = \"BSBI\", \n",
    "                 postings_encoding = None):\n",
    "        self.term_id_map = IdMap()\n",
    "        self.doc_id_map = IdMap()\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding\n",
    "\n",
    "        # Stores names of intermediate indices\n",
    "        self.intermediate_indices = []\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Dumps doc_id_map and term_id_map into output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'wb') as f:\n",
    "            pkl.dump(self.term_id_map, f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'wb') as f:\n",
    "            pkl.dump(self.doc_id_map, f)\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Loads doc_id_map and term_id_map from output directory\"\"\"\n",
    "        \n",
    "        with open(os.path.join(self.output_dir, 'terms.dict'), 'rb') as f:\n",
    "            self.term_id_map = pkl.load(f)\n",
    "        with open(os.path.join(self.output_dir, 'docs.dict'), 'rb') as f:\n",
    "            self.doc_id_map = pkl.load(f)\n",
    "            \n",
    "    def index(self):\n",
    "        \"\"\"Base indexing code\n",
    "        \n",
    "        This function loops through the data directories, \n",
    "        calls parse_block to parse the documents\n",
    "        calls invert_write, which inverts each block and writes to a new index\n",
    "        then saves the id maps and calls merge on the intermediate indices\n",
    "        \"\"\"\n",
    "        for block_dir_relative in sorted(next(os.walk(self.data_dir))[1]):\n",
    "            td_pairs = self.parse_block(block_dir_relative)\n",
    "            index_id = 'index_'+block_dir_relative\n",
    "            self.intermediate_indices.append(index_id)\n",
    "            with InvertedIndexWriter(index_id, directory=self.output_dir, \n",
    "                                     postings_encoding=\n",
    "                                     self.postings_encoding) as index:\n",
    "                self.invert_write(td_pairs, index)\n",
    "                td_pairs = None\n",
    "        self.save()\n",
    "        with InvertedIndexWriter(self.index_name, directory=self.output_dir, \n",
    "                                 postings_encoding=\n",
    "                                 self.postings_encoding) as merged_index:\n",
    "            with contextlib.ExitStack() as stack:\n",
    "                indices = [stack.enter_context(\n",
    "                    InvertedIndexIterator(index_id, \n",
    "                                          directory=self.output_dir, \n",
    "                                          postings_encoding=\n",
    "                                          self.postings_encoding)) \n",
    "                 for index_id in self.intermediate_indices]\n",
    "                self.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing\n",
    "\n",
    "> The function `parse_block`  parses documents into termID-docID pairs and accumulates the pairs in memory until a block of a fixed size is full. We choose the block size to fit comfortably into memory to permit a fast in-memory sort. \n",
    "\n",
    "You should treat each sub-directory as a block; `parse_block` expects the path to the sub-directory which is the block. You should assume that the individual file names are unique within each sub-directory, but not necessarily the case across sub- directories (i.e., the full path of the files are unique).\n",
    "\n",
    "_NB - We are inheriting `BSBIIndex` from `BSBIIndex`, which is just a simple way of adding a new method to an existing class. Please do not be confused, it is simply used as a pedagogical tool to split a class definition within a jupyter notebook and there is no other magic happening here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/parse_block.py\n",
    "class BSBIIndex(BSBIIndex):            \n",
    "    def parse_block(self, block_dir_relative):\n",
    "        \"\"\"Parses a tokenized text file into termID-docID pairs\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        block_dir_relative : str\n",
    "            Relative Path to the directory that contains the files for the block\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[Int, Int]]\n",
    "            Returns all the td_pairs extracted from the block\n",
    "        \n",
    "        Should use self.term_id_map and self.doc_id_map to get termIDs and docIDs.\n",
    "        These persist across calls to parse_block\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the function works as expected on the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('toy-data/0/fine.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "with open('toy-data/0/hello.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'tmp/', index_name = 'toy')\n",
    "BSBI_instance.parse_block('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the `parse_block` method work as expected on a block of the toy data? Write some tests to make sure that is indeed the case (for example, you should make sure that a given word gets the same id each time it appears)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion\n",
    "\n",
    "> The block is then inverted and written to disk. Inversion involves two steps. First, we sort the termID-docID pairs. Next, we collect all termID-docID pairs with the same termID into a postings list, where a posting is simply a docID. The result, an inverted index for the block we have just read, is then written to disk.\n",
    "\n",
    "In this section we add the function `invert_write` which does exactly this. \n",
    "\n",
    "However, we first need to implement the class `InvertedIndexWriter`. This class provides an append function, just like a list, however the postings_list isn't stored in memory but is directly written to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/inverted_index_writer.py\n",
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')              \n",
    "        return self\n",
    "\n",
    "    def append(self, term, postings_list):\n",
    "        \"\"\"Appends the term and postings_list to end of the index file.\n",
    "        \n",
    "        This function does three things, \n",
    "        1. Encodes the postings_list using self.postings_encoding\n",
    "        2. Stores metadata in the form of self.terms and self.postings_dict\n",
    "           Note that self.postings_dict maps termID to a 3 tuple of \n",
    "           (start_position_in_index_file, \n",
    "           number_of_postings_in_list, \n",
    "           length_in_bytes_of_postings_list)\n",
    "        3. Appends the bytestream to the index file on disk\n",
    "\n",
    "        Hint: You might find it helpful to read the Python I/O docs\n",
    "        (https://docs.python.org/3/tutorial/inputoutput.html) for\n",
    "        information about appending to the end of a file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        term:\n",
    "            term or termID is the unique identifier for the term\n",
    "        postings_list: List[Int]\n",
    "            List of docIDs where the term appears\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we haven't written the classes which read the index, we can still test the implementation as follows. Make sure you pass the asserts before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InvertedIndexWriter('test', directory='tmp/') as index:\n",
    "    index.append(1, [2, 3, 4])\n",
    "    index.append(2, [3, 4, 5])\n",
    "    index.index_file.seek(0)\n",
    "    assert index.terms == [1,2], \"terms sequence incorrect\"\n",
    "    assert index.postings_dict == {1: (0, 3, len(UncompressedPostings.encode([2,3,4]))), \n",
    "                                   2: (len(UncompressedPostings.encode([2,3,4])), 3, \n",
    "                                       len(UncompressedPostings.encode([3,4,5])))}, \"postings_dict incorrect\"\n",
    "    assert UncompressedPostings.decode(index.index_file.read()) == [2, 3, 4, 3, 4, 5], \"postings on disk incorrect\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement `invert_write`, which takes in the td_pairs created with parse_block. We use `InvertedIndexWriter` to write them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/invert_write.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def invert_write(self, td_pairs, index):\n",
    "        \"\"\"Inverts td_pairs into postings_lists and writes them to the given index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        td_pairs: List[Tuple[Int, Int]]\n",
    "            List of termID-docID pairs\n",
    "        index: InvertedIndexWriter\n",
    "            Inverted index on disk corresponding to the block       \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this on the toy data, we can read a block and see what the inverted index contains. Write some tests like you saw for the `InvertedIndexWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "> The algorithm simultaneously merges the ten blocks into one large merged index. To do the merging, we open all block files simultaneously, and maintain small read buffers for the ten blocks we are reading and a write buffer for the final merged index we are writing. \n",
    "\n",
    "The iterator paradigm in Python lends itself naturally to our need of maintaining a small read buffer. We can iterate through the file while reading just one postings list at a time from the disk. We subclass `InvertedIndex` into `InvertedIndexIterator` which does the iteration bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/inverted_index_iterator.py\n",
    "class InvertedIndexIterator(InvertedIndex):\n",
    "    \"\"\"\"\"\"\n",
    "    def __enter__(self):\n",
    "        \"\"\"Adds an initialization_hook to the __enter__ function of super class\n",
    "        \"\"\"\n",
    "        super().__enter__()\n",
    "        self._initialization_hook()\n",
    "        return self\n",
    "\n",
    "    def _initialization_hook(self):\n",
    "        \"\"\"Use this function to initialize the iterator\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def __iter__(self): \n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Returns the next (term, postings_list) pair in the index.\n",
    "        \n",
    "        Note: This function should only read a small amount of data from the \n",
    "        index file. In particular, you should not try to maintain the full \n",
    "        index file in memory.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "    def delete_from_disk(self):\n",
    "        \"\"\"Marks the index for deletion upon exit. Useful for temporary indices\n",
    "        \"\"\"\n",
    "        self.delete_upon_exit = True\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        \"\"\"Delete the index file upon exiting the context along with the\n",
    "        functions of the super class __exit__ function\"\"\"\n",
    "        self.index_file.close()\n",
    "        if hasattr(self, 'delete_upon_exit') and self.delete_upon_exit:\n",
    "            os.remove(self.index_file_path)\n",
    "            os.remove(self.metadata_file_path)\n",
    "        else:\n",
    "            with open(self.metadata_file_path, 'wb') as f:\n",
    "                pkl.dump([self.postings_dict, self.terms], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by first creating an index with the `InvertedIndexWriter` and then iterating through it. Write a few small test cases to see if it works. At the very least, you should write a test which manually constructs a small inverted index, writes them to disk with an `InvertedIndexWriter`, then uses an `InvertedIndexIterator` to iterate over the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> During merging, in each iteration, we select the lowest termID that has not been processed yet using a priority queue or a similar data structure. All postings lists for this termID are read and merged, and the merged list is written back to disk. Each read buffer is refilled from its file when necessary.\n",
    "\n",
    "We'll use the `InvertedIndexIterator` to do the reading part and `InvertedIndexWriter` to write the merged postings list. \n",
    "\n",
    "Note that we've been opening each inverted index using `with` statement, but now we need to do that programmatically. You can see that we've used `contextlib` ([documentation](https://docs.python.org/3/library/contextlib.html#contextlib.ExitStack)) for that in the provided `index` function of `BSBIIndex` base class. Your task is to **write the logic** for merging *opened* `InvertedIndexIterator` objects and writing one postings list at a time into into a single `InvertedIndexWriter` object. \n",
    "\n",
    "Since we know that the postings are sorted, we can appropriately merge them in sorted order in linear time. In fact `heapq` ([documentation](https://docs.python.org/3.0/library/heapq.html)) is a standard python module that provides an implementation of the heap queue algorithm. In particular it contains a `heapq.merge` utility function which merges multiple sorted inputs into a single sorted output and returns an iterator over the sorted values. Not only can this be handy with merging postings lists, but also with merging the inverted indices. \n",
    "\n",
    "To get you started on using the `heapq.merge` function, we've provided a sample usage of the function. The two lists contain animals/birds sorted by their average life span. We want to merge the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "animal_lifespans = [('Giraffe', 28), \n",
    "                   ('Rhinoceros', 40), \n",
    "                   ('Indian Elephant', 70), \n",
    "                   ('Golden Eagle', 80), \n",
    "                   ('Box turtle', 123)]\n",
    "\n",
    "tree_lifespans = [('Gray Birch', 50), \n",
    "                  ('Black Willow', 70), \n",
    "                  ('Basswood', 100),\n",
    "                  ('Bald Cypress', 600)]\n",
    "\n",
    "lifespan_lists = [animal_lifespans, tree_lifespans]\n",
    "\n",
    "for merged_item in heapq.merge(*lifespan_lists, key=lambda x: x[1]):\n",
    "    print(merged_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `*` to unpack `lifespan_lists` as arguments and `lambda` function to reprsent the key to perform the soring with. If you are unfamiliar you can check documentation ([unpacking lists](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists), [lambda expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)) or tutorials ([unpacking lists](https://www.geeksforgeeks.org/packing-and-unpacking-arguments-in-python/), [lambda_expressions](https://www.afternerd.com/blog/python-lambdas/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/merge.py\n",
    "\n",
    "import heapq\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def merge(self, indices, merged_index):\n",
    "        \"\"\"Merges multiple inverted indices into a single index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        indices: List[InvertedIndexIterator]\n",
    "            A list of InvertedIndexIterator objects, each representing an\n",
    "            iterable inverted index for a block\n",
    "        merged_index: InvertedIndexWriter\n",
    "            An instance of InvertedIndexWriter object into which each merged \n",
    "            postings list is written out one at a time\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make sure it works without errors on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir=toy_dir, output_dir = 'toy_output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets index the full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're having errors with just the merging part of it, you can use the following code to debug it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.intermediate_indices = ['index_'+str(i) for i in range(10)]\n",
    "with InvertedIndexWriter(BSBI_instance.index_name, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding) as merged_index:\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        indices = [stack.enter_context(InvertedIndexIterator(index_id, directory=BSBI_instance.output_dir, postings_encoding=BSBI_instance.postings_encoding)) for index_id in BSBI_instance.intermediate_indices]\n",
    "        BSBI_instance.merge(indices, merged_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean conjunctive retrieval\n",
    "\n",
    "We'll be implementing a function `retrieve` to BSBIIndex, which given a query string consisting of space separated tokens, returns a list of documents that contain each of the tokens in the query. However, we do not want to be iterating through the index nor loading the entire index to find the relevant terms. \n",
    "\n",
    "First we'll implement `InvertedIndexMapper` which subclasses `InvertedIndex` to add functionality for retrieving postings corresponding to a particular term by seeking to that location in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/inverted_index_mapper.py\n",
    "class InvertedIndexMapper(InvertedIndex):\n",
    "    def __getitem__(self, key):\n",
    "        return self._get_postings_list(key)\n",
    "    \n",
    "    def _get_postings_list(self, term):\n",
    "        \"\"\"Gets a postings list (of docIds) for `term`.\n",
    "        \n",
    "        This function should not iterate through the index file.\n",
    "        I.e., it should only have to read the bytes from the index file\n",
    "        corresponding to the postings list for the requested term.\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few tests to check your `_get_postings_list` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can obtain postings lists corresponding to the query terms, we want to intersect them. However, Ã  la merging, we can use the sorted-ness of these lists to intersect them very efficiently. Here we'll implement `sorted_intersect` which takes two sorted lists and return a sorted intersection of the elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/sorted_intersect.py\n",
    "def sorted_intersect(list1, list2):\n",
    "    \"\"\"Intersects two (ascending) sorted lists and returns the sorted result\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: List[Comparable]\n",
    "    list2: List[Comparable]\n",
    "        Sorted lists to be intersected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Comparable]\n",
    "        Sorted intersection        \n",
    "    \"\"\"\n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few simple test cases to make sure it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the `retrieve` function using `InvertedIndexMapper` and `sorted_intersect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/retrieve.py\n",
    "class BSBIIndex(BSBIIndex):\n",
    "    def retrieve(self, query):\n",
    "        \"\"\"Retrieves the documents corresponding to the conjunctive query\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "            Space separated list of query tokens\n",
    "            \n",
    "        Result\n",
    "        ------\n",
    "        List[str]\n",
    "            Sorted list of documents which contains each of the query tokens. \n",
    "            Should be empty if no documents are found.\n",
    "        \n",
    "        Should NOT throw errors for terms not in corpus\n",
    "        \"\"\"\n",
    "        if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "            self.load()\n",
    "\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test if our index works on the real corpus with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir', )\n",
    "BSBI_instance.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also test if one of these pages truly contains the query terms by reading a file as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pa1-data/1/cs276.stanford.edu_\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test on the dev queries to see if it works alright. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an assert is failing you can compare the difference in outputs as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(my_results) - set(reference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(reference_results) - set(my_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you write your own queries test for all kinds of corner cases like terms which have never occurred in the corpus, or terms that are very frequent and might slow down the merge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading breakdown\n",
    "To ensure your index is built correctly, you will be tested on 20 Boolean conjunctive queries of one or multiple terms. For those queries, you will get 1.5% of the final grade for each query you answer correctly, for a total of 30% of your grade. 8 of those queries will be the same as the provided dev queries. \n",
    "\n",
    "Note that we'll be running your submission in an underpowered VM (to simulate pairity with realistic conditions where you would have larger block sizes and more memory) with around 750 MB of available memory. If your program uses more than that, it will be killed by the VM. You should make sure well ahead of time that your submission works and passes the visible tests and queries upon submitting to the gradescope autograder.\n",
    "\n",
    "Further we'll also run a few tests to check if the intermediate and final index files generated match the reference implementation and also a few unit tests to check correctness of each of the functions asked to implement. Note that due to the ordering constraints there is exactly one correct output expected. You will earn the remaining 15% of the points based on that. The breakdown is as follows\n",
    "\n",
    "1. `IdMap` - 2%\n",
    "2. `parse_block` - 2%\n",
    "3. `InvertedIndexWriter` - 1%\n",
    "4. `invert_write` - 2%\n",
    "5. `InvertedIndexIterator` - 1%\n",
    "6. `merge` - 2%\n",
    "7. `InvertedIndexMapper` - 1%\n",
    "8. `sorted_intersect` - 2%\n",
    "9. `retrieve` - 2%\n",
    "\n",
    "We will also collect timing statistics on how long the indexing takes, the size of your generated index, and the amount of memory used when building the index. \n",
    "Finally we'll inspect the code, generated indices and penalize by\n",
    "1. 5% if your `retrieve` function loads the whole index into memory rather than simply loading the postings lists of just the query terms.\n",
    "2. 5% if your `merge` function loads the complete intermediate indices or keeps the whole merged index in memory.\n",
    "3. 4% if you do not order the query terms by postings list length in `retrieve` function.\n",
    "4. 4% if you use the built in set operations (which do not explicitly use the sorteness of postings lists) while merging postings list in `merge` function or intersecting sorted lists in `sorted_intersect` function. \n",
    "5. 5% if the timing statistics of your retrieval algorithm (`retrieve` function) are way out of the norm (which might happen if you iterate through your index instead of seeking just the query terms).\n",
    "6. 5% if the memory sizes of generated indices are way out of the norm \n",
    "\n",
    "The maximum combined grades for this section is 45%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a compressed index (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you should build a compressed index using gap encoding with variable byte encoding for each gap.\n",
    "\n",
    "Here are some things about python you'll find useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remainder (modulo) operator %\n",
    "\n",
    "print(\"10 % 2 = \", 10 % 2)\n",
    "print(\"10 % 3 = \", 10 % 3)\n",
    "\n",
    "# Integer division in Python 3 is done by using two slash signs\n",
    "\n",
    "print(\"10 / 3 = \", 10 / 3)\n",
    "print(\"10 // 3 = \", 10 // 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following `CompressedPostings` class which we shall use as a drop-in replacement for `UncompressedPostings`. You should refer to [Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf) of the textbook for understanding how gap encoding with variable byte encoding for each gap is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/compressed_postings.py\n",
    "class CompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` using gap encoding with variable byte \n",
    "        encoding for each gap\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "            (as produced by `array.tobytes` function)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docIds)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write some test cases for any helper methods that you might have implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a simple helper function for you to test whether an encoded postings list is decode correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode(l):\n",
    "    e = CompressedPostings.encode(l)\n",
    "    d = CompressedPostings.decode(e)\n",
    "    assert d == l\n",
    "    print(l, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a few test cases to make sure the postings list compression and decompression is being done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a new output directory and use the `CompressedPostings` to create a compressed index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.mkdir('output_dir_compressed')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSBI_instance_compressed = BSBIIndex(data_dir='pa1-data', output_dir = 'output_dir_compressed', postings_encoding=CompressedPostings)\n",
    "BSBI_instance_compressed.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let us now test on the dev queries to see if it works alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9):\n",
    "    with open('dev_queries/query.' + str(i)) as q:\n",
    "        query = q.read()\n",
    "        my_results = [os.path.normpath(path) for path in BSBI_instance_compressed.retrieve(query)]\n",
    "        with open('dev_output/' + str(i) + '.out') as o:\n",
    "            reference_results = [os.path.normpath(x.strip()) for x in o.readlines()]\n",
    "            assert my_results == reference_results, \"Results DO NOT match for query: \"+query.strip()\n",
    "        print(\"Results match for query:\", query.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading breakdown\n",
    "\n",
    "Similar to uncompressed indexing, you will be tested on 20 conjunctive queries (1.5% for each query you\n",
    "answer correctly, for a total of 30%). \n",
    "\n",
    "You will be penalized by:\n",
    "1. 15% (of the total grade for the assignment) if your compression algorithm does not achieve a compressed index of size less than 20MB on disk (just the .index file without the .dict file).\n",
    "2. 5% (of the total grade for the assignment) if your query response time with the compressed index is way out of norm.\n",
    "\n",
    "The combined total grade of this section is 30%.   \n",
    "*Note that if your program does not implement the variable length encoding compression algorithm, you will receive no credit for this task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code submission\n",
    "\n",
    "Now you are ready to submit the code part of your assignment. Refer to [submission instructions section](#Submission-instructions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (25%)\n",
    "\n",
    "## Running time analysis (10%)\n",
    "\n",
    "In this section your task is to write queries to probe running time characteristics of `retrieve` function (without compresssion) and use it to illustrate key steps and optimizations of the retrieval algorithm. \n",
    "\n",
    "Each of the following subsections have two deliverables, \n",
    "1. One or more queries \n",
    "2. Reasoning in the form of a written answer. Please limit your answer to 3 sentences.  \n",
    "\n",
    "Each subsection is worth 2% of the assignment. \n",
    "\n",
    "**Written answers should be blockquotes**, so that we don't miss them while grading. You can edit a markdown cell by double clicking on it. See the last cell of the following sub-section for an example.\n",
    "\n",
    "To get you started, we have provided with an example of timing a query (Note that the following cells assume you have an indexed BSBIIndex object)\n",
    "\n",
    "You can read more about `%timeit` in the [documentation](https://docs.python.org/3/library/timeit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit BSBI_instance.retrieve('boolean retrieval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `q1` - Slower query (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you replace a term in the query so that it runs slower? Time the query and see if it behaves as per your hypothesis. We will refer to this query as `q1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your reasoning behind the change? Why did the retrieve function slow down?**\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `q2` - Twice as slow query (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you make the retrieval twice as slow? Construct a query that runs twice as slow as your previous query (`q1`). We will refer to this query as `q2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the general principle behind your construction of the query that made it run twice as slowly?**\n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `q3` - Confounding factors (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query that ran twice as slow (`q2`) might differ from its previous query (`q1`) in many ways. Thus the slowdown could have been caused due to any of those differences. \n",
    "\n",
    "Write one or more queries to show that that the above stated general principle is the sole cause for the slowdown and none of the other differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What factors could you eliminate as a reason for slowdown? Describe how the queries eliminated those factors.** \n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Were there are any factors that you couldn't eliminate? Can you change `q2` so that it is still twice as slow as `q1` but no longer has confounding factors? If not, do you need to rethink your general principle for slowdown in `q2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `q4` - Adding new terms (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a new query by adding a term to `q2` that makes it go faster than `q2` (though it'll likely be much slower than `q1`). We'll refer to it as `q4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did you do it? What is the principle behind the increase in speed?** \n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `q5` - Effect of shuffling (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the terms in `q4` and time it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you notice a *significant* difference between timing for `q4` and `q5`? Why would you not expect the timing to change too much?** \n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these timings are not perfect but you should be able to get significant differences where you would expect them without much effort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index size (6%) \n",
    "\n",
    "In this section we will look at the size of index on disk. Each subsection is worth 2% of the assignment. We will be grading both the analysis code and the written answer.\n",
    "\n",
    "### Uncompressed index (2%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the size of the merged index (`.index`) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of index\", os.path.getsize(\"output_dir/BSBI.index\"), 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above queries the file system to compute the size of the index. Show that you can compute the same answer by looking at the `postings_dict` object directly. In particular, write one line of Python code to compute the size of the index file (in bytes) using the postings dict.\n",
    "\n",
    "**Hint:** You should use the word size (in bytes) and size information stored in the `postings_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_dir/BSBI.dict', 'rb') as f:\n",
    "    postings_dict, terms = pkl.load(f)\n",
    "    \n",
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did you compute the expected size based on the information stored in the postings dictionary? What is the size of each integer representing a posting?**\n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing size of uncompressed index (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of reducing size of uncompressed index is to simply use fewer bits for each posting. What is the smallest number of bits that you would need to represent postings (i.e. docIds) for this document collection? In the following cell collect statistics that help you determine the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin your code\n",
    "\n",
    "### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If each posting was represented by the number of bits you calculated above, what is the expected size of the uncompressed index? How did you arrive a that number?**  \n",
    "You can use the previous cell to perform additional calculations\n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Index (2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the size of the compressed index (`.index`) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of compressed index\", os.path.getsize(\"output_dir_compressed/BSBI.index\"), 'bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the compressed index smaller in size than the one we estimated in the previous section? Why?\n",
    "How would the size of the compressed index change if variable byte encoding was used on the docIDs instead of the gaps?**\n",
    "\n",
    "> *Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving performance (9%)\n",
    "\n",
    "In this section you'll write a few sentences (about 5) to answer each of the following questions. Each question is worth 3% of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "**In this PA we asked you to use each sub-directory as a block and build an index for one block at a time. Can you discuss the trade-off of different sizes of blocks? Is there a general strategy when we are working with limited memory but want to minimize indexing time?**\n",
    "\n",
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "**Is there a part of your indexing program that limits its scalability to larger datasets?**\n",
    "\n",
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "**Describe other parts of the indexing process that you can optimize for indexing time/scalability and retrieval time.**\n",
    "\n",
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report submission \n",
    "You are now ready to submit your report as well. Refer to [submission instructions section](#Submission-instructions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional compression methods (10% - Extra credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one more index compression method by filling in the `encode` and `decode` methods in `ECCompressedPostings`. \n",
    "\n",
    "Here are a few directions  \n",
    "1. [Section 5.4](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf) in the textbook points to a few techniques (e.g., **gamma-encoding**) and related publications. \n",
    "2. You can also look into **Delta Encoding**, a compression technique very similar to Gamma Encoding, which can achieve an even better compression rate. \n",
    "3. Also, [ALGORITHM SIMPLE-9](https://github.com/manning/CompressionAlgorithms#simple-9) is an interesting encoding technique that you could also consider implementing.\n",
    "\n",
    "You should store each postings list is stored in a multiple of bytes (rather than bits). The positions and lengths in index file are in multiples of bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tee submission/ec_compressed_postings.py\n",
    "class ECCompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "\n",
    "    ### End your code\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        \"\"\"Encodes `postings_list` \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        postings_list: List[int]\n",
    "            The postings list to be encoded\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bytes: \n",
    "            Bytes reprsentation of the compressed postings list \n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        \"\"\"Decodes a byte representation of compressed postings list\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_postings_list: bytes\n",
    "            Bytes representation as produced by `CompressedPostings.encode` \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Decoded postings list (each posting is a docId)\n",
    "        \"\"\"\n",
    "        ### Begin your code\n",
    "\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a description of your index compression method as well as a discussion of the space/speed trade-off of this additional compression method, in comparison to what we implemented in previous tasks.**\n",
    "\n",
    "> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be awarded 8% for answering 20 queries correctly (0.4% each). 2% will be given for the space/speed discussion in your report. \n",
    "\n",
    "Note that since the objective of this task is to achieve better compression rate, you'll receive 0% out of 10% if the size of your compressed index is larger than the one using `CompressedPostings`. We will compare against our implementation of `CompressedPostings`, but the results of the autograder tests will be kept visible so you'll know whether you achieve a better compression (and hence the extra credit) before the submission deadline. \n",
    "\n",
    "You will be penalized if the retrieval times are abnormally long. We will inspect your code for implementation correctness if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC submission\n",
    "\n",
    "You are now ready to submit extra-credit as well. You will have to resubmit all code and complete PDF. Refer to [submission instructions section](#Submission-instructions). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
